{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9702c9-4016-42a2-89fc-6b45b926eec0",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fd5f297-7b57-4222-a34c-e727ac38b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from collections import Counter\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb10e23-fa8e-409a-9436-d6065ad56878",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f1e0bef-55de-42fc-a7d3-30b29bc82e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "      this class will later get the following attributes\n",
    "      all nodes:\n",
    "          features\n",
    "          responses\n",
    "      split nodes additionally:\n",
    "          left\n",
    "          right\n",
    "          split_index\n",
    "          threshold\n",
    "      leaf nodes additionally:\n",
    "          prediction\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "class Tree(ABC):\n",
    "    '''\n",
    "      base class for RegressionTree and ClassificationTree\n",
    "    '''\n",
    "    def __init__(self, n_min=10):\n",
    "        '''n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        self.n_min = n_min \n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' return the predictions for the given feature matrix X\n",
    "        '''\n",
    "        if X.ndim == 1:\n",
    "            return self._predict_one(X)\n",
    "        else:\n",
    "            return np.array([self._predict_one(x) for x in X])\n",
    "        \n",
    "    def _predict_one(self, x):\n",
    "        ''' return the prediction for a single 1-D feature vector x\n",
    "        '''\n",
    "        node = self.root\n",
    "        while not hasattr(node, \"prediction\"):\n",
    "            j = node.split_index\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.prediction\n",
    "        \n",
    "    def train(self, features, responses, D_try=None):\n",
    "        '''\n",
    "        features: the feature matrix of the training set\n",
    "        response: the vector of responses\n",
    "        '''\n",
    "        N, D = features.shape\n",
    "        assert(responses.shape[0] == N)\n",
    "\n",
    "        if D_try is None:\n",
    "            D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "        \n",
    "        # initialize the root node\n",
    "        self.root = Node()\n",
    "        self.root.features  = features\n",
    "        self.root.responses = responses\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            active_indices = self.select_active_indices(D, D_try)\n",
    "            left, right = self.make_split_node(node, active_indices)\n",
    "            if left is None: # no split found\n",
    "                self.make_leaf_node(node)\n",
    "            else:\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "    \n",
    "    def make_split_node(self, node, indices):\n",
    "        '''\n",
    "        node: the node to be split\n",
    "        indices: a numpy array of length 'D_try', containing the feature \n",
    "                         indices to be considered for the present split\n",
    "                         \n",
    "        return: None, None -- if no suitable split has been found, or\n",
    "                left, right -- the children of the split\n",
    "        '''\n",
    "        # all responses equal => no improvement possible by any split\n",
    "        if np.unique(node.responses).shape[0] == 1:\n",
    "            return None, None\n",
    "        \n",
    "        # find best feature j_min (among 'indices') and best threshold t_min for the split\n",
    "        l_min = float('inf')  # upper bound for the loss, later the loss of the best split\n",
    "        j_min, t_min = None, None\n",
    "\n",
    "        for j in indices:\n",
    "            thresholds = self.find_thresholds(node, j)\n",
    "\n",
    "            # compute loss for each threshold\n",
    "            for t in thresholds:\n",
    "                loss = self.compute_loss_for_split(node, j, t)\n",
    "\n",
    "                # remember the best split so far \n",
    "                # (the condition is never True when loss = float('inf') )\n",
    "                if loss < l_min:\n",
    "                    l_min = loss\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "        if j_min is None: # no split found\n",
    "            return None, None\n",
    "\n",
    "        # create children for the best split\n",
    "        left, right = self.make_children(node, j_min, t_min)\n",
    "\n",
    "        # turn the current 'node' into a split node\n",
    "        # (store children and split condition)\n",
    "        node.split_index = j_min\n",
    "        node.threshold = t_min\n",
    "        node.left = left\n",
    "        node.right = right\n",
    "        \n",
    "        # return the children (to be placed on the stack)\n",
    "        return left, right\n",
    "    \n",
    "    def select_active_indices(self, D, D_try):\n",
    "        ''' return a 1-D array with D_try randomly selected indices from 0...(D-1).\n",
    "        '''\n",
    "        return np.random.choice(D, D_try, replace=False)\n",
    "        \n",
    "    def find_thresholds(self, node, j):\n",
    "        ''' return: a 1-D array with all possible thresholds along feature j\n",
    "        '''\n",
    "        feature_values = node.features[:, j]\n",
    "        unique_values = np.unique(feature_values)\n",
    "        thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "        return thresholds\n",
    "        \n",
    "    def make_children(self, node, j, t):\n",
    "        ''' execute the split in feature j at threshold t\n",
    "        \n",
    "            return: left, right -- the children of the split, with features and responses\n",
    "                                   properly assigned according to the split\n",
    "        '''\n",
    "        left = Node()\n",
    "        right = Node()\n",
    "\n",
    "        mask_left = node.features[:, j] <= t\n",
    "        mask_right = ~mask_left\n",
    "\n",
    "        left.features = node.features[mask_left]\n",
    "        left.responses = node.responses[mask_left]\n",
    "\n",
    "        right.features = node.features[mask_right]\n",
    "        right.responses = node.responses[mask_right]\n",
    "        \n",
    "        return left, right\n",
    "        \n",
    "    @abstractmethod\n",
    "    def make_leaf_node(self, node):\n",
    "        ''' Turn node into a leaf by computing and setting `node.prediction`\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"make_leaf_node() must be implemented in a subclass.\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        ''' Return the resulting loss when the data are split along feature j at threshold t.\n",
    "            If the split is not admissible, return float('inf').\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"compute_loss_for_split() must be implemented in a subclass.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbcb6b9-03ac-4c2e-9ba2-ac9a1435b90f",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d9f27e1-29d5-4e50-90fd-c2bc45618f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(Tree, BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_min=10):\n",
    "        super(RegressionTree, self).__init__(n_min)\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        mask_left = node.features[:, j] <= t\n",
    "        mask_right = ~mask_left\n",
    "\n",
    "        if np.sum(mask_left) < self.n_min or np.sum(mask_right) < self.n_min:\n",
    "            return float('inf')\n",
    "\n",
    "        left_responses = node.responses[mask_left]\n",
    "        right_responses = node.responses[mask_right]\n",
    "\n",
    "        loss_left = np.var(left_responses) * len(left_responses)\n",
    "        loss_right = np.var(right_responses) * len(right_responses)\n",
    "\n",
    "        return loss_left + loss_right\n",
    "        \n",
    "    def make_leaf_node(self, node):\n",
    "        node.prediction = np.mean(node.responses)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.train(X, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73dd6b-d3ad-47c8-b21b-ffefa2e0253e",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7020dd0-fc3e-45f1-b532-fb4e5fc110ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree(Tree, BaseEstimator, ClassifierMixin):\n",
    "    '''implement classification tree so that it can handle arbitrary many classes\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, classes, n_min=10):\n",
    "        ''' classes: a 1-D array with the permitted class labels\n",
    "            n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        super(ClassificationTree, self).__init__(n_min)\n",
    "        self.classes = classes\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        mask_left = node.features[:, j] <= t\n",
    "        mask_right = ~mask_left\n",
    "\n",
    "        if np.sum(mask_left) < self.n_min or np.sum(mask_right) < self.n_min:\n",
    "            return float('inf')\n",
    "\n",
    "        left_responses = node.responses[mask_left]\n",
    "        right_responses = node.responses[mask_right]\n",
    "\n",
    "        loss_left = self.gini_impurity(left_responses) * len(left_responses)\n",
    "        loss_right = self.gini_impurity(right_responses) * len(right_responses)\n",
    "\n",
    "        return loss_left + loss_right\n",
    "    \n",
    "    def gini_impurity(self, responses):\n",
    "        counts = np.array(list(Counter(responses).values()))\n",
    "        probabilities = counts / len(responses)\n",
    "        return 1.0 - np.sum(probabilities ** 2)\n",
    "\n",
    "    def make_leaf_node(self, node):\n",
    "        counts = Counter(node.responses)\n",
    "        node.prediction = max(counts, key=counts.get)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.train(X, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40082f0-2065-4c30-8a23-a84eaa85d2e2",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c79d6863-93e7-4ef1-ba23-43f2949ee2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "RegressionTree MSE: 0.3655716218501607\n",
      "ClassificationTree Accuracy: 0.9173896499238964\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the digits data and extract 3s and 9s\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    "\n",
    "instances = (digits.target == 3) | (digits.target == 9)\n",
    "features = digits.data[instances, :]\n",
    "labels = digits.target[instances]\n",
    "\n",
    "# For regression, we use labels +1 and -1\n",
    "responses = np.array([1 if l == 3 else -1 for l in labels])\n",
    "\n",
    "assert(features.shape[0] == labels.shape[0] == responses.shape[0])\n",
    "\n",
    "# Perform 5-fold cross-validation with responses +1 and -1 (for 3s and 9s) using RegressionTree()\n",
    "regression_tree = RegressionTree(n_min=2)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "regression_scores = cross_val_score(regression_tree, features, responses, cv=kf, scoring='neg_mean_squared_error')\n",
    "print(f\"RegressionTree MSE: {-np.mean(regression_scores)}\")\n",
    "\n",
    "# Perform 5-fold cross-validation with labels 3 and 9 using ClassificationTree\n",
    "classification_tree = ClassificationTree(classes=np.unique(labels), n_min=2)\n",
    "classification_scores = cross_val_score(classification_tree, features, labels, cv=kf, scoring='accuracy')\n",
    "print(f\"ClassificationTree Accuracy: {np.mean(classification_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086425e9-bf50-4555-8317-f4386019a6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
